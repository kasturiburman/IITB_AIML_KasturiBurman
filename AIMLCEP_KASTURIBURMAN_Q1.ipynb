$\large{\text{Nearest Neighbor Classification}}$

We assume that the training features are from some input space $\mathcal{X} \subseteq {\mathbb{R}}^d$. Note that $\mathcal{X}$ is a subset of the Euclidean space. Hence distance metrics used for Euclidean space can be used to compare the closeness of two points $x=(x_1,x_2,\ldots,x_d)$ and $z=(z_1,z_2,\ldots,z_d)$ in $\mathcal{X}$.   

Recall that the following metrics are some examples of distance metrics in $\mathcal{X}$:




*   $\ell_2$ distance between $x$ and $z$ given by: $\|x-z\|_2 = \sqrt{\sum_{i=1}^{d} (x_i - z_i)^2}$
*   $\ell_1$ distance between $x$ and $z$ given by: $\|x-z\|_1 = \sum_{i=1}^{d} |x_i - z_i|$
*   $\ell_\infty$ distance between $x$ and $z$ given by: $\|x-z\|_\infty = \max_{i \in \{1,2,\ldots,d\}} |x_i - z_i|$
*   $\ell_p$ distance between $x$ and $z$ given by: $\|x-z\|_p =  (\sum_{i=1}^{d} |x_i - z_i|^p)^\frac{1}{p}$, where $1 \leq p < \infty$.

Unlike previous algorithms that we have seen like SVM, Decision tree or logistic regression, nearest neighbor algorithm does not go for a model building activity immediately when the training data is available. 

Typically the model building activity using the training data set is $\textbf{inductive}$ in nature, since the model is induced using the training data. 

Nearest neighbor algorithm on the other hand is a $\textbf{lazy}$ learner in the sense that it waits until a test instance is available for inference. 

Recall that in algorithms like SVM, logistic regression and decision trees, the inference is a $\textbf{deductive}$ process since the label is deduced from the model. 

However in nearest neighbor algorithm, this deduction process is not carried out using a learned model as such. Instead the model is built and used on-the-fly to make the inference. 




